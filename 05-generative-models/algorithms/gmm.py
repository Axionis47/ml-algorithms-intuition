"""
GAUSSIAN MIXTURE MODEL — Paradigm: GENERATIVE (Latent Variables)

===============================================================
WHAT IT IS (THE CORE IDEA)
===============================================================

Model the data as coming from a MIXTURE of Gaussians:

    p(x) = Σₖ πₖ N(x | μₖ, Σₖ)

Each data point was generated by:
    1. Pick a cluster k with probability πₖ
    2. Sample x from Gaussian N(μₖ, Σₖ)

This is a GENERATIVE model: we model how the data was made,
not just how to classify it.

===============================================================
THE KEY INSIGHT: LATENT VARIABLES & EM
===============================================================

The cluster assignment z is a LATENT (hidden) variable.
If we knew z for each point, fitting would be trivial (just MLE).
But we don't!

EM (Expectation-Maximization) iterates:

    E-step: Compute responsibility γₙₖ = P(z=k | xₙ)
            "How much does cluster k explain point n?"

    M-step: Update parameters using soft assignments
            μₖ = weighted mean of points (weighted by γₙₖ)
            Σₖ = weighted covariance
            πₖ = fraction of responsibility

This converges to a LOCAL maximum of likelihood.

===============================================================
GMM vs K-MEANS
===============================================================

K-MEANS:
    - Hard assignment: each point belongs to exactly one cluster
    - Spherical clusters (Euclidean distance)
    - Just cluster centers, no shape

GMM:
    - Soft assignment: points have probability of belonging to each
    - Elliptical clusters (covariance captures shape)
    - Full probabilistic model (can sample, compute likelihoods)

GMM is the "probabilistic K-means" — it's a proper generative model.

===============================================================
COVARIANCE TYPES
===============================================================

Full: Each cluster has its own full covariance matrix
    - Most flexible (ellipsoids of any orientation)
    - Most parameters (d² per cluster)

Diagonal: Covariance is diagonal (axis-aligned ellipsoids)
    - Less flexible
    - Fewer parameters (d per cluster)

Spherical: Σ = σ²I (spheres, like K-means)
    - Least flexible
    - Fewest parameters (1 per cluster)

===============================================================
INDUCTIVE BIAS
===============================================================

1. Data is a mixture of Gaussians (strong assumption!)
2. Clusters are ellipsoidal
3. Each cluster has a "weight" (prior probability)
4. Soft assignment respects uncertainty

===============================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.insert(0, '/Users/sid47/ML Algorithms')
from importlib import import_module
datasets_module = import_module('00_datasets')
accuracy = datasets_module.accuracy


def create_gmm_dataset(n_samples=500, n_clusters=3, d=2, random_state=42):
    """Create a dataset from a true GMM."""
    np.random.seed(random_state)

    # True parameters
    n_per_cluster = n_samples // n_clusters

    X_list = []
    y_list = []

    # Generate cluster centers
    centers = np.random.randn(n_clusters, d) * 3

    for k in range(n_clusters):
        # Random covariance (ensure positive definite)
        A = np.random.randn(d, d) * 0.5
        cov = A @ A.T + 0.1 * np.eye(d)

        # Sample points
        X_k = np.random.multivariate_normal(centers[k], cov, n_per_cluster)
        X_list.append(X_k)
        y_list.append(np.full(n_per_cluster, k))

    X = np.vstack(X_list)
    y = np.concatenate(y_list)

    # Shuffle
    perm = np.random.permutation(len(X))
    return X[perm], y[perm]


class GaussianMixtureModel:
    """
    Gaussian Mixture Model with EM algorithm.
    """

    def __init__(self, n_components=3, covariance_type='full',
                 max_iter=100, tol=1e-4, random_state=None):
        """
        Parameters:
        -----------
        n_components : Number of Gaussian components (clusters)
        covariance_type : 'full', 'diag', or 'spherical'
        max_iter : Maximum EM iterations
        tol : Convergence tolerance (log-likelihood change)
        """
        self.n_components = n_components
        self.covariance_type = covariance_type
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state

        # Parameters (set during fit)
        self.weights_ = None      # πₖ (mixing coefficients)
        self.means_ = None        # μₖ (cluster means)
        self.covariances_ = None  # Σₖ (cluster covariances)

    def _initialize(self, X):
        """Initialize parameters."""
        n_samples, n_features = X.shape
        K = self.n_components

        if self.random_state is not None:
            np.random.seed(self.random_state)

        # Initialize means with random data points
        indices = np.random.choice(n_samples, K, replace=False)
        self.means_ = X[indices].copy()

        # Initialize covariances
        if self.covariance_type == 'full':
            # Use data covariance as starting point
            data_cov = np.cov(X.T) + 0.1 * np.eye(n_features)
            self.covariances_ = np.array([data_cov.copy() for _ in range(K)])
        elif self.covariance_type == 'diag':
            data_var = np.var(X, axis=0) + 0.1
            self.covariances_ = np.array([data_var.copy() for _ in range(K)])
        else:  # spherical
            data_var = np.var(X) + 0.1
            self.covariances_ = np.full(K, data_var)

        # Initialize uniform weights
        self.weights_ = np.full(K, 1.0 / K)

    def _compute_log_likelihood(self, X):
        """
        Compute log-likelihood of data under current parameters.

        log p(X) = Σₙ log Σₖ πₖ N(xₙ | μₖ, Σₖ)
        """
        n_samples = X.shape[0]
        K = self.n_components

        # Compute log probability of each point under each component
        log_probs = np.zeros((n_samples, K))

        for k in range(K):
            # Pass raw covariance (not transformed by _get_covariance)
            log_probs[:, k] = self._log_gaussian_pdf(X, self.means_[k],
                                                      self.covariances_[k])

        # Add log weights
        log_probs += np.log(self.weights_)

        # Log-sum-exp for numerical stability
        max_log_probs = np.max(log_probs, axis=1, keepdims=True)
        log_likelihood = np.sum(max_log_probs.squeeze() +
                               np.log(np.sum(np.exp(log_probs - max_log_probs), axis=1)))

        return log_likelihood

    def _log_gaussian_pdf(self, X, mean, cov):
        """
        Compute log N(x | mean, cov) for each x in X.
        """
        n_samples, d = X.shape

        if self.covariance_type == 'full':
            # Full covariance
            diff = X - mean
            try:
                L = np.linalg.cholesky(cov)
                log_det = 2 * np.sum(np.log(np.diag(L)))
                solve = np.linalg.solve(L, diff.T).T
                mahal = np.sum(solve ** 2, axis=1)
            except np.linalg.LinAlgError:
                # Fallback if not positive definite
                cov = cov + 0.01 * np.eye(d)
                L = np.linalg.cholesky(cov)
                log_det = 2 * np.sum(np.log(np.diag(L)))
                solve = np.linalg.solve(L, diff.T).T
                mahal = np.sum(solve ** 2, axis=1)

        elif self.covariance_type == 'diag':
            # Diagonal covariance
            diff = X - mean
            log_det = np.sum(np.log(cov))
            mahal = np.sum(diff ** 2 / cov, axis=1)

        else:  # spherical
            diff = X - mean
            log_det = d * np.log(cov)
            mahal = np.sum(diff ** 2, axis=1) / cov

        log_prob = -0.5 * (d * np.log(2 * np.pi) + log_det + mahal)
        return log_prob

    def _get_covariance(self, k):
        """Get covariance matrix for component k."""
        if self.covariance_type == 'full':
            return self.covariances_[k]
        elif self.covariance_type == 'diag':
            return np.diag(self.covariances_[k])
        else:  # spherical
            d = self.means_.shape[1]
            return self.covariances_[k] * np.eye(d)

    def _e_step(self, X):
        """
        E-step: Compute responsibilities.

        γₙₖ = P(zₙ = k | xₙ) = πₖ N(xₙ | μₖ, Σₖ) / Σⱼ πⱼ N(xₙ | μⱼ, Σⱼ)
        """
        n_samples = X.shape[0]
        K = self.n_components

        # Compute log probabilities
        log_probs = np.zeros((n_samples, K))
        for k in range(K):
            # Pass raw covariance (not transformed by _get_covariance)
            log_probs[:, k] = self._log_gaussian_pdf(X, self.means_[k],
                                                      self.covariances_[k])
        log_probs += np.log(self.weights_)

        # Softmax for responsibilities (numerically stable)
        max_log_probs = np.max(log_probs, axis=1, keepdims=True)
        log_sum = max_log_probs + np.log(np.sum(np.exp(log_probs - max_log_probs), axis=1, keepdims=True))
        log_responsibilities = log_probs - log_sum

        return np.exp(log_responsibilities)

    def _m_step(self, X, responsibilities):
        """
        M-step: Update parameters using responsibilities.

        Nₖ = Σₙ γₙₖ  (effective number of points in cluster k)
        μₖ = (1/Nₖ) Σₙ γₙₖ xₙ
        Σₖ = (1/Nₖ) Σₙ γₙₖ (xₙ - μₖ)(xₙ - μₖ)ᵀ
        πₖ = Nₖ / N
        """
        n_samples, d = X.shape
        K = self.n_components

        # Effective number of points per cluster
        N_k = np.sum(responsibilities, axis=0) + 1e-10

        # Update weights
        self.weights_ = N_k / n_samples

        # Update means
        self.means_ = (responsibilities.T @ X) / N_k[:, np.newaxis]

        # Update covariances
        if self.covariance_type == 'full':
            for k in range(K):
                diff = X - self.means_[k]
                weighted_diff = responsibilities[:, k:k+1] * diff
                self.covariances_[k] = (weighted_diff.T @ diff) / N_k[k]
                # Regularize for numerical stability
                self.covariances_[k] += 1e-6 * np.eye(d)

        elif self.covariance_type == 'diag':
            for k in range(K):
                diff = X - self.means_[k]
                self.covariances_[k] = np.sum(responsibilities[:, k:k+1] * diff ** 2, axis=0) / N_k[k]
                self.covariances_[k] += 1e-6

        else:  # spherical
            for k in range(K):
                diff = X - self.means_[k]
                self.covariances_[k] = np.sum(responsibilities[:, k:k+1] * np.sum(diff ** 2, axis=1, keepdims=True)) / (N_k[k] * d)
                self.covariances_[k] += 1e-6

    def fit(self, X, verbose=True):
        """
        Fit GMM using EM algorithm.
        """
        self._initialize(X)

        log_likelihoods = []
        prev_ll = -np.inf

        for iteration in range(self.max_iter):
            # E-step
            responsibilities = self._e_step(X)

            # M-step
            self._m_step(X, responsibilities)

            # Compute log-likelihood
            ll = self._compute_log_likelihood(X)
            log_likelihoods.append(ll)

            if verbose and (iteration + 1) % 10 == 0:
                print(f"Iteration {iteration+1}: log-likelihood = {ll:.4f}")

            # Check convergence
            if abs(ll - prev_ll) < self.tol:
                if verbose:
                    print(f"Converged at iteration {iteration+1}")
                break

            prev_ll = ll

        self.log_likelihoods_ = log_likelihoods
        return self

    def predict(self, X):
        """Predict cluster labels (hard assignment)."""
        responsibilities = self._e_step(X)
        return np.argmax(responsibilities, axis=1)

    def predict_proba(self, X):
        """Predict cluster probabilities (soft assignment)."""
        return self._e_step(X)

    def score_samples(self, X):
        """Compute log-likelihood of each sample."""
        n_samples = X.shape[0]
        K = self.n_components

        log_probs = np.zeros((n_samples, K))
        for k in range(K):
            log_probs[:, k] = self._log_gaussian_pdf(X, self.means_[k],
                                                      self._get_covariance(k))
        log_probs += np.log(self.weights_)

        # Log-sum-exp
        max_log_probs = np.max(log_probs, axis=1, keepdims=True)
        return max_log_probs.squeeze() + np.log(np.sum(np.exp(log_probs - max_log_probs), axis=1))

    def sample(self, n_samples=1):
        """Generate samples from the fitted model."""
        # Sample cluster assignments
        z = np.random.choice(self.n_components, size=n_samples, p=self.weights_)

        # Sample from each cluster
        d = self.means_.shape[1]
        X = np.zeros((n_samples, d))

        for k in range(self.n_components):
            mask = z == k
            n_k = np.sum(mask)
            if n_k > 0:
                cov = self._get_covariance(k)
                X[mask] = np.random.multivariate_normal(self.means_[k], cov, n_k)

        return X, z


# ============================================================
# ABLATION EXPERIMENTS
# ============================================================

def ablation_experiments():
    print("\n" + "="*60)
    print("ABLATION EXPERIMENTS")
    print("="*60)

    np.random.seed(42)
    X, y_true = create_gmm_dataset(n_samples=600, n_clusters=3)

    # -------- Experiment 1: Number of Components --------
    print("\n1. EFFECT OF NUMBER OF COMPONENTS")
    print("-" * 40)
    print("What happens when K doesn't match true clusters?")

    for n_comp in [2, 3, 4, 5, 6]:
        gmm = GaussianMixtureModel(n_components=n_comp, random_state=42)
        gmm.fit(X, verbose=False)
        ll = gmm._compute_log_likelihood(X)
        bic = -2 * ll + n_comp * (1 + 2*2 + 3) * np.log(len(X))  # Approximate BIC
        print(f"K={n_comp} log_likelihood={ll:.1f} BIC≈{bic:.1f}")
    print("→ More components = higher likelihood (always)")
    print("→ BIC penalizes complexity → finds true K")

    # -------- Experiment 2: Covariance Types --------
    print("\n2. EFFECT OF COVARIANCE TYPE")
    print("-" * 40)

    for cov_type in ['full', 'diag', 'spherical']:
        gmm = GaussianMixtureModel(n_components=3, covariance_type=cov_type,
                                   random_state=42)
        gmm.fit(X, verbose=False)
        ll = gmm._compute_log_likelihood(X)

        # Count parameters
        d = X.shape[1]
        if cov_type == 'full':
            n_params = 3 * (d + d*(d+1)//2 + 1)
        elif cov_type == 'diag':
            n_params = 3 * (d + d + 1)
        else:
            n_params = 3 * (d + 1 + 1)

        print(f"cov_type={cov_type:<10} params={n_params:<4} log_likelihood={ll:.1f}")
    print("→ Full covariance captures cluster shape best")
    print("→ Spherical = like K-means (assumes round clusters)")

    # -------- Experiment 3: Initialization Sensitivity --------
    print("\n3. INITIALIZATION SENSITIVITY (Multiple Runs)")
    print("-" * 40)
    log_likelihoods = []
    for seed in range(10):
        gmm = GaussianMixtureModel(n_components=3, random_state=seed)
        gmm.fit(X, verbose=False)
        ll = gmm._compute_log_likelihood(X)
        log_likelihoods.append(ll)

    print(f"Log-likelihoods across 10 runs:")
    print(f"  Min: {min(log_likelihoods):.1f}")
    print(f"  Max: {max(log_likelihoods):.1f}")
    print(f"  Mean: {np.mean(log_likelihoods):.1f}")
    print(f"  Std: {np.std(log_likelihoods):.1f}")
    print("→ EM finds LOCAL maxima → initialization matters!")

    # -------- Experiment 4: GMM vs K-Means --------
    print("\n4. GMM vs K-MEANS (Soft vs Hard Assignment)")
    print("-" * 40)

    # Create overlapping clusters
    np.random.seed(42)
    X_overlap = np.vstack([
        np.random.randn(100, 2) + [0, 0],
        np.random.randn(100, 2) + [2, 0],  # Close to first cluster
    ])
    y_overlap = np.array([0]*100 + [1]*100)

    gmm = GaussianMixtureModel(n_components=2, random_state=42)
    gmm.fit(X_overlap, verbose=False)

    # Get soft assignments
    probs = gmm.predict_proba(X_overlap)

    # Count uncertain points (neither probability > 0.9)
    uncertain_mask = np.max(probs, axis=1) < 0.9
    n_uncertain = np.sum(uncertain_mask)

    print(f"Points with max probability < 0.9: {n_uncertain} / {len(X_overlap)}")
    print(f"Average max probability: {np.mean(np.max(probs, axis=1)):.3f}")
    print("→ GMM captures UNCERTAINTY in cluster assignment!")
    print("→ K-means forces hard decisions even for ambiguous points")

    # -------- Experiment 5: Sampling (Generative Capability) --------
    print("\n5. SAMPLING FROM FITTED MODEL")
    print("-" * 40)

    gmm = GaussianMixtureModel(n_components=3, random_state=42)
    gmm.fit(X, verbose=False)

    X_sampled, z_sampled = gmm.sample(100)

    print(f"Original data mean:  {X.mean(axis=0)}")
    print(f"Sampled data mean:   {X_sampled.mean(axis=0)}")
    print(f"Original data std:   {X.std(axis=0)}")
    print(f"Sampled data std:    {X_sampled.std(axis=0)}")
    print("→ GMM is a GENERATIVE model → can synthesize new data!")

    # -------- Experiment 6: Log-Likelihood Convergence --------
    print("\n6. EM CONVERGENCE")
    print("-" * 40)

    gmm = GaussianMixtureModel(n_components=3, max_iter=50, random_state=42)
    gmm.fit(X, verbose=False)

    print("Log-likelihood progression:")
    for i, ll in enumerate(gmm.log_likelihoods_[:10]):
        print(f"  Iteration {i+1}: {ll:.1f}")
    print("  ...")
    print(f"  Final ({len(gmm.log_likelihoods_)}): {gmm.log_likelihoods_[-1]:.1f}")
    print("→ Log-likelihood ALWAYS increases (EM guarantee)")
    print("→ Converges to LOCAL maximum")


def visualize_gmm():
    """Visualize GMM clustering."""
    print("\n" + "="*60)
    print("GMM VISUALIZATION")
    print("="*60)

    np.random.seed(42)
    X, y_true = create_gmm_dataset(n_samples=500, n_clusters=3)

    fig, axes = plt.subplots(2, 3, figsize=(12, 8))

    # Row 1: Different numbers of components
    for i, n_comp in enumerate([2, 3, 4]):
        ax = axes[0, i]
        gmm = GaussianMixtureModel(n_components=n_comp, random_state=42)
        gmm.fit(X, verbose=False)
        y_pred = gmm.predict(X)

        scatter = ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.5, s=20)

        # Plot cluster means
        ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1],
                  c='red', marker='X', s=200, edgecolors='black')

        # Draw ellipses for covariances
        for k in range(n_comp):
            cov = gmm._get_covariance(k)
            eigenvalues, eigenvectors = np.linalg.eigh(cov)
            angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
            width, height = 2 * np.sqrt(eigenvalues)

            from matplotlib.patches import Ellipse
            ellipse = Ellipse(gmm.means_[k], width*2, height*2, angle=angle,
                            fill=False, color='red', linewidth=2)
            ax.add_patch(ellipse)

        ll = gmm._compute_log_likelihood(X)
        ax.set_title(f'K={n_comp}, LL={ll:.1f}')

    # Row 2: Different covariance types
    for i, cov_type in enumerate(['full', 'diag', 'spherical']):
        ax = axes[1, i]
        gmm = GaussianMixtureModel(n_components=3, covariance_type=cov_type,
                                   random_state=42)
        gmm.fit(X, verbose=False)
        y_pred = gmm.predict(X)

        ax.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.5, s=20)
        ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1],
                  c='red', marker='X', s=200, edgecolors='black')

        # Draw ellipses
        for k in range(3):
            cov = gmm._get_covariance(k)
            eigenvalues, eigenvectors = np.linalg.eigh(cov)
            angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
            width, height = 2 * np.sqrt(eigenvalues)

            from matplotlib.patches import Ellipse
            ellipse = Ellipse(gmm.means_[k], width*2, height*2, angle=angle,
                            fill=False, color='red', linewidth=2)
            ax.add_patch(ellipse)

        ax.set_title(f'Covariance: {cov_type}')

    plt.suptitle('GAUSSIAN MIXTURE MODEL\n'
                 'Red X = means, Ellipses = 2σ covariance contours',
                 fontsize=12)
    plt.tight_layout()
    return fig


def benchmark_clustering():
    """Benchmark GMM on clustering accuracy."""
    print("\n" + "="*60)
    print("BENCHMARK: Clustering Accuracy")
    print("="*60)

    results = {}

    for n_clusters in [2, 3, 4]:
        X, y_true = create_gmm_dataset(n_samples=600, n_clusters=n_clusters)

        gmm = GaussianMixtureModel(n_components=n_clusters, random_state=42)
        gmm.fit(X, verbose=False)
        y_pred = gmm.predict(X)

        # Compute clustering accuracy (best permutation)
        from itertools import permutations
        best_acc = 0
        for perm in permutations(range(n_clusters)):
            y_mapped = np.array([perm[y] for y in y_pred])
            acc = np.mean(y_mapped == y_true)
            best_acc = max(best_acc, acc)

        results[n_clusters] = best_acc
        print(f"K={n_clusters} clusters: accuracy={best_acc:.3f}")

    return results


if __name__ == '__main__':
    print("="*60)
    print("GMM — Gaussian Mixture Model")
    print("="*60)

    print("""
WHAT THIS MODEL IS:
    p(x) = Σₖ πₖ N(x | μₖ, Σₖ)

    Data = mixture of Gaussians.
    Each point was generated by picking a cluster, then sampling.

THE KEY INSIGHT:
    Cluster assignment z is a LATENT VARIABLE.
    EM iterates: E-step (compute responsibilities) ↔ M-step (update params)

GMM vs K-MEANS:
    K-means: hard assignment, spherical clusters
    GMM: soft assignment (probabilities), elliptical clusters

COVARIANCE TYPES:
    Full: most flexible (any ellipsoid)
    Diagonal: axis-aligned ellipsoids
    Spherical: like K-means

GENERATIVE:
    GMM is a GENERATIVE model — can sample new data!
    """)

    ablation_experiments()
    results = benchmark_clustering()

    fig = visualize_gmm()
    save_path = '/Users/sid47/ML Algorithms/16_gmm.png'
    fig.savefig(save_path, dpi=100, bbox_inches='tight')
    print(f"\nSaved to: {save_path}")
    plt.close(fig)

    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print("""
1. GMM models data as mixture of K Gaussians
2. EM algorithm: E-step (responsibilities) ↔ M-step (update)
3. Soft assignment captures uncertainty (vs K-means hard)
4. Covariance type controls cluster shape flexibility
5. EM finds LOCAL maximum → initialization matters
6. GENERATIVE: can sample new data from fitted model
    """)
